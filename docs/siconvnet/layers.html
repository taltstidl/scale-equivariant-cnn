<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>siconvnet.layers API documentation</title>
<meta name="description" content="This module contains the layers used by the scale-invariant models. Currently there are three: â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>siconvnet.layers</code></h1>
</header>
<section id="section-intro">
<p>This module contains the layers used by the scale-invariant models. Currently there are three:</p>
<ul>
<li><code><a title="siconvnet.layers.Interpolate" href="#siconvnet.layers.Interpolate">Interpolate</a></code>: Implements the interpolation of the kernels.</li>
<li><code><a title="siconvnet.layers.SiConv2d" href="#siconvnet.layers.SiConv2d">SiConv2d</a></code>: Provides a replacement for <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code>Conv2d</code></a> that
uses scale-invariant convolutions. Note that only the input interface is compatible, as the output contains an
additional dimension for the scales.</li>
<li><code><a title="siconvnet.layers.ScalePool" href="#siconvnet.layers.ScalePool">ScalePool</a></code>: Provides a method to remove the additional dimension using maximum pooling.</li>
</ul>
<p>These layers make it easy to create your own models employing scale-invariant convolutions.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># pylint: disable=no-member
&#34;&#34;&#34;
This module contains the layers used by the scale-invariant models. Currently there are three:

* `Interpolate`: Implements the interpolation of the kernels.
* `SiConv2d`: Provides a replacement for [`Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) that
  uses scale-invariant convolutions. Note that only the input interface is compatible, as the output contains an
  additional dimension for the scales.
* `ScalePool`: Provides a method to remove the additional dimension using maximum pooling.

These layers make it easy to create your own models employing scale-invariant convolutions.
&#34;&#34;&#34;
import math

import torch
import torch.nn.functional as F
from torch import nn


class Interpolate(nn.Module):
    &#34;&#34;&#34; Kernel interpolation layer.

    To implement scale-invariant convolutions, a reference kernel that stores the shared weights needs to be scaled to
    larger (or possibly smaller) sizes. As the sizes do not match, interpolation needs to be applied.

    Parameters
    ----------
    kernel_size: int
        Input kernel size of the given reference kernel.
    target_size: int
        Targeted kernel size for the scaled kernel.
    mode: Literal[&#39;nearest&#39;, &#39;bilinear&#39;, &#39;bicubic&#39;, &#39;area&#39;]
        Method used for interpolation. Must be either nearest, bilinear or bicubic. Internally used the functional
        [`interpolate`](https://pytorch.org/docs/stable/nn.functional.html#interpolate) method provided by PyTorch.
    &#34;&#34;&#34;

    def __init__(self, kernel_size: int, target_size: int, mode: str = &#39;bilinear&#39;):
        &#34;&#34;&#34;&#34;&#34;&#34;
        super().__init__()
        if mode not in [&#39;nearest&#39;, &#39;bilinear&#39;, &#39;bicubic&#39;, &#39;area&#39;]:
            raise ValueError(
                &#39;Interpolation mode must be either `nearest`, `bilinear`, `bicubic` or `slice`, got {}&#39;.format(mode))
        self.kernel_size = kernel_size
        self.target_size = target_size
        self.mode = mode
        if self.mode == &#39;area&#39;:
            interpolation_matrix = self._generate_interpolation_matrix(kernel_size, target_size)
            self.register_buffer(&#39;interpolation_matrix&#39;, interpolation_matrix, persistent=False)

    def forward(self, x):
        &#34;&#34;&#34;
        Interpolates the kernels using the given interpolation method. In addition, it normalizes the rescaled kernel
        such that the outputs remain comparable across scales, i.e. by multiplying with \\(k_{in}^2 / k_{out}^2\\)
        where \\(k_{in}\\) is the input kernel size and \\(k_{out}\\) is the targeted kernel size.

        Parameters
        ----------
        x: torch.Tensor
            Kernels that should be interpolated, of shape (out_channels, in_channels, kernel_size, kernel_size)

        Returns
        -------
        torch.Tensor
            Rescaled kernels, of shape (out_channels, in_channels, target_size, target_size)
        &#34;&#34;&#34;
        if self.mode == &#39;area&#39;:
            out_channels, in_channels, kernel_h, kernel_w = x.shape
            x = x.view(out_channels, in_channels, kernel_h * kernel_w, 1)
            x = torch.matmul(self.interpolation_matrix, x)
            x = x.view(out_channels, in_channels, self.target_size, self.target_size)
        else:
            x = F.interpolate(x, size=(self.target_size, self.target_size), mode=self.mode)
        x *= (self.kernel_size * self.kernel_size) / (self.target_size * self.target_size)
        return x

    @staticmethod
    def _generate_interpolation_matrix(kernel_size, target_size):
        &#34;&#34;&#34; Generates an interpolation matrix for use with area-based interpolation.

        Parameters
        ----------
        kernel_size: int
            Input kernel size of the given reference kernel.
        target_size: int
            Targeted kernel size for the scaled kernel.

        Returns
        -------
        torch.Tensor
            Interpolation matrix, of shape (target_size**2, kernel_size**2)
        &#34;&#34;&#34;
        # Generate interpolation for up-scaling to size m
        matrix = torch.zeros(target_size * target_size, kernel_size * kernel_size)
        # i and j are indices into the up-scaled kernel
        for i in range(target_size):
            ix = (i * kernel_size) // target_size
            id = max(0, (i + 1) * kernel_size - (ix + 1) * target_size)
            for j in range(target_size):
                jx = (j * kernel_size) // target_size
                jd = max(0, (j + 1) * kernel_size - (jx + 1) * target_size)
                if id == 0 and jd == 0:
                    matrix[i * target_size + j, ix * kernel_size + jx] = 1.0
                elif id == 0:
                    jfrac = jd / kernel_size
                    matrix[i * target_size + j, ix * kernel_size + jx] = 1.0 - jfrac
                    matrix[i * target_size + j, ix * kernel_size + (jx + 1)] = jfrac
                elif jd == 0:
                    ifrac = id / kernel_size
                    matrix[i * target_size + j, ix * kernel_size + jx] = 1.0 - ifrac
                    matrix[i * target_size + j, (ix + 1) * kernel_size + jx] = ifrac
                else:
                    ifrac, jfrac = id / kernel_size, jd / kernel_size
                    matrix[i * target_size + j, ix * kernel_size + jx] = (1.0 - ifrac) * (1.0 - jfrac)
                    matrix[i * target_size + j, ix * kernel_size + (jx + 1)] = jfrac * (1.0 - ifrac)
                    matrix[i * target_size + j, (ix + 1) * kernel_size + jx] = ifrac * (1.0 - jfrac)
                    matrix[i * target_size + j, (ix + 1) * kernel_size + (jx + 1)] = ifrac * jfrac
        return matrix


class SiConv2d(nn.Module):
    &#34;&#34;&#34; Scale-invariant convolution layer.

    Applies a scale-invariant 2D convolution over an input signal composed of several
    input planes. This operation is comparable to a standard convolution, but generates an
    additional output dimension representing scale.

    Parameters
    ----------
    in_channels: int
        Number of channels in the input image.
    out_channels: int
        Number of channels produced by the convolution.
    num_scales: int
        Number of scales the kernels are applied on. Starts with kernel size and then incrementally
        increases by 2 on each side. Usually chosen so largest kernel size covers complete input.
    kernel_size: int
        Size of the convolution kernel. Only kernels with the same height and width are supported.
    padding: bool
        Whether zero-padding should be applied to the input. If enabled, significantly more operations
        are performed which might degrade performance.
    interp_mode: Literal[&#39;nearest&#39;, &#39;bilinear&#39;, &#39;bicubic&#39;, &#39;area&#39;]
        Method used for kernel interpolation. See `Interpolate` for details.
    &#34;&#34;&#34;

    def __init__(self, in_channels: int, out_channels: int, num_scales: int, kernel_size: int, stride: int = 1,
                 padding: bool = False, interp_mode: str = &#39;nearest&#39;):
        &#34;&#34;&#34;&#34;&#34;&#34;
        super().__init__()
        # Save all arguments as instance variables
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_scales = num_scales
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.interp_mode = interp_mode
        # Generate weights and bias
        weight = torch.empty((out_channels, in_channels, kernel_size, kernel_size))
        self.weights = nn.Parameter(weight)
        bias = torch.empty((out_channels,))
        self.bias = nn.Parameter(bias)
        # Initialize weights and bias
        stdv = 1.0 / math.sqrt(in_channels * kernel_size * kernel_size)
        self.weights.data.uniform_(-stdv, stdv)
        self.bias.data.uniform_(-stdv, stdv)
        # Initialize interpolation layers
        interpolate = list()
        for i in range(num_scales):
            interpolate.append(Interpolate(kernel_size, kernel_size + 2 * i, mode=interp_mode))
        self.interpolate = nn.ModuleList(interpolate)

    def forward(self, x):
        &#34;&#34;&#34; Performs the scale-invariant convolution on the input.
        &#34;&#34;&#34;
        # Assert shape matches expectations
        _, in_channels, height, width = x.shape
        assert in_channels == self.in_channels, &#39;Expected {} input channels, got {}&#39; \
            .format(self.in_channels, in_channels)
        num_scales = (min(height, width) - self.kernel_size) // 2 + 1
        assert num_scales == self.num_scales, &#39;Expected {} scales, got {}&#39; \
            .format(self.num_scales, num_scales)
        # Apply scale-invariant convolution
        # 1. Generate scaled kernel for each scale
        kernels = list()
        for interpolate in self.interpolate:
            kernels.append(interpolate(self.weights))
        # 2. Compute convolution for each scale
        outputs = list()
        for kernel in kernels:
            if self.padding:
                _padding = kernel.shape[-1] // 2
                output = F.conv2d(x, kernel, self.bias, padding=_padding)
            else:
                output = F.conv2d(x, kernel, self.bias)
                _padding = (kernel.shape[-1] // 2) - (kernels[0].shape[-1] // 2)
                output = F.pad(output, 4 * (_padding,))
            outputs.append(output)
        return torch.stack(outputs, 2)


class ScalePool(nn.Module):
    &#34;&#34;&#34; Pooling layer to collapse the additional scale dimension.

    One of the problems after applying the scale-invariant convolution is the additional dimension representing the
    scale. This makes it incompatible with many standard architectures and increases the overall complexity. One way
    to handle this is to remove the dimension via pooling. Two pooling strategies are proposed:

    * Pixel-wise Pooling (`pixel`): This is considered the standard pooling operation, whereby each pixel is pooled
    independently of each other over the scale domain. However, this can lead to uncoordinated activations as the
    scales for each pixel don&#39;t necessarily match or correlate.
    * Slice-wise Pooling (`slice`): This is an alternative to the standard pooling operation, where the slice
    containing the largest activation is used. Since the scales for each pixel are thus equal, the activations are
    better coordinated.

    Parameters
    ----------
    mode: Literal[&#39;pixel&#39;, &#39;slice&#39;]
        Method used for pooling. Must be either pixel or slice. For the former, the maximum for each individual pixel
        is returned, for the latter the slice with the largest activation is returned.
    &#34;&#34;&#34;

    def __init__(self, mode: str = &#39;pixel&#39;):
        &#34;&#34;&#34;&#34;&#34;&#34;
        super().__init__()
        if mode not in [&#39;pixel&#39;, &#39;slice&#39;]:
            raise ValueError(&#39;Pooling mode must be either `pixel` or `slice`, got {}&#39;.format(mode))
        self.mode = mode

    def forward(self, x):
        &#34;&#34;&#34; Performs the pooling on the input.

        Parameters
        ----------
        x: torch.Tensor
            Feature maps that should be pooled, of shape (mini_batch, in_channels, num_scales, height, width)

        Returns
        -------
        torch.Tensor
            Pooled feature maps, of shape (mini_batch, in_channels, height, width)
        &#34;&#34;&#34;
        if self.mode == &#39;pixel&#39;:
            num_scales = x.shape[-3]
            return F.max_pool3d(x, kernel_size=(num_scales, 1, 1)).squeeze(dim=-3)
        if self.mode == &#39;slice&#39;:
            num_scales, height, width = x.shape[-3], x.shape[-2], x.shape[-1]
            _, act_index = F.max_pool3d(x, kernel_size=(num_scales, height, width), return_indices=True)
            indices = act_index.repeat(1, 1, 1, height, width) // (height * width)
            return x.gather(dim=-3, index=indices).squeeze(dim=-3)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="siconvnet.layers.Interpolate"><code class="flex name class">
<span>class <span class="ident">Interpolate</span></span>
<span>(</span><span>kernel_size:Â int, target_size:Â int, mode:Â strÂ =Â 'bilinear')</span>
</code></dt>
<dd>
<div class="desc"><p>Kernel interpolation layer.</p>
<p>To implement scale-invariant convolutions, a reference kernel that stores the shared weights needs to be scaled to
larger (or possibly smaller) sizes. As the sizes do not match, interpolation needs to be applied.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Input kernel size of the given reference kernel.</dd>
<dt><strong><code>target_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Targeted kernel size for the scaled kernel.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>Literal['nearest', 'bilinear', 'bicubic', 'area']</code></dt>
<dd>Method used for interpolation. Must be either nearest, bilinear or bicubic. Internally used the functional
<a href="https://pytorch.org/docs/stable/nn.functional.html#interpolate"><code>interpolate</code></a> method provided by PyTorch.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Interpolate(nn.Module):
    &#34;&#34;&#34; Kernel interpolation layer.

    To implement scale-invariant convolutions, a reference kernel that stores the shared weights needs to be scaled to
    larger (or possibly smaller) sizes. As the sizes do not match, interpolation needs to be applied.

    Parameters
    ----------
    kernel_size: int
        Input kernel size of the given reference kernel.
    target_size: int
        Targeted kernel size for the scaled kernel.
    mode: Literal[&#39;nearest&#39;, &#39;bilinear&#39;, &#39;bicubic&#39;, &#39;area&#39;]
        Method used for interpolation. Must be either nearest, bilinear or bicubic. Internally used the functional
        [`interpolate`](https://pytorch.org/docs/stable/nn.functional.html#interpolate) method provided by PyTorch.
    &#34;&#34;&#34;

    def __init__(self, kernel_size: int, target_size: int, mode: str = &#39;bilinear&#39;):
        &#34;&#34;&#34;&#34;&#34;&#34;
        super().__init__()
        if mode not in [&#39;nearest&#39;, &#39;bilinear&#39;, &#39;bicubic&#39;, &#39;area&#39;]:
            raise ValueError(
                &#39;Interpolation mode must be either `nearest`, `bilinear`, `bicubic` or `slice`, got {}&#39;.format(mode))
        self.kernel_size = kernel_size
        self.target_size = target_size
        self.mode = mode
        if self.mode == &#39;area&#39;:
            interpolation_matrix = self._generate_interpolation_matrix(kernel_size, target_size)
            self.register_buffer(&#39;interpolation_matrix&#39;, interpolation_matrix, persistent=False)

    def forward(self, x):
        &#34;&#34;&#34;
        Interpolates the kernels using the given interpolation method. In addition, it normalizes the rescaled kernel
        such that the outputs remain comparable across scales, i.e. by multiplying with \\(k_{in}^2 / k_{out}^2\\)
        where \\(k_{in}\\) is the input kernel size and \\(k_{out}\\) is the targeted kernel size.

        Parameters
        ----------
        x: torch.Tensor
            Kernels that should be interpolated, of shape (out_channels, in_channels, kernel_size, kernel_size)

        Returns
        -------
        torch.Tensor
            Rescaled kernels, of shape (out_channels, in_channels, target_size, target_size)
        &#34;&#34;&#34;
        if self.mode == &#39;area&#39;:
            out_channels, in_channels, kernel_h, kernel_w = x.shape
            x = x.view(out_channels, in_channels, kernel_h * kernel_w, 1)
            x = torch.matmul(self.interpolation_matrix, x)
            x = x.view(out_channels, in_channels, self.target_size, self.target_size)
        else:
            x = F.interpolate(x, size=(self.target_size, self.target_size), mode=self.mode)
        x *= (self.kernel_size * self.kernel_size) / (self.target_size * self.target_size)
        return x

    @staticmethod
    def _generate_interpolation_matrix(kernel_size, target_size):
        &#34;&#34;&#34; Generates an interpolation matrix for use with area-based interpolation.

        Parameters
        ----------
        kernel_size: int
            Input kernel size of the given reference kernel.
        target_size: int
            Targeted kernel size for the scaled kernel.

        Returns
        -------
        torch.Tensor
            Interpolation matrix, of shape (target_size**2, kernel_size**2)
        &#34;&#34;&#34;
        # Generate interpolation for up-scaling to size m
        matrix = torch.zeros(target_size * target_size, kernel_size * kernel_size)
        # i and j are indices into the up-scaled kernel
        for i in range(target_size):
            ix = (i * kernel_size) // target_size
            id = max(0, (i + 1) * kernel_size - (ix + 1) * target_size)
            for j in range(target_size):
                jx = (j * kernel_size) // target_size
                jd = max(0, (j + 1) * kernel_size - (jx + 1) * target_size)
                if id == 0 and jd == 0:
                    matrix[i * target_size + j, ix * kernel_size + jx] = 1.0
                elif id == 0:
                    jfrac = jd / kernel_size
                    matrix[i * target_size + j, ix * kernel_size + jx] = 1.0 - jfrac
                    matrix[i * target_size + j, ix * kernel_size + (jx + 1)] = jfrac
                elif jd == 0:
                    ifrac = id / kernel_size
                    matrix[i * target_size + j, ix * kernel_size + jx] = 1.0 - ifrac
                    matrix[i * target_size + j, (ix + 1) * kernel_size + jx] = ifrac
                else:
                    ifrac, jfrac = id / kernel_size, jd / kernel_size
                    matrix[i * target_size + j, ix * kernel_size + jx] = (1.0 - ifrac) * (1.0 - jfrac)
                    matrix[i * target_size + j, ix * kernel_size + (jx + 1)] = jfrac * (1.0 - ifrac)
                    matrix[i * target_size + j, (ix + 1) * kernel_size + jx] = ifrac * (1.0 - jfrac)
                    matrix[i * target_size + j, (ix + 1) * kernel_size + (jx + 1)] = ifrac * jfrac
        return matrix</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="siconvnet.layers.Interpolate.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Interpolates the kernels using the given interpolation method. In addition, it normalizes the rescaled kernel
such that the outputs remain comparable across scales, i.e. by multiplying with <span><span class="MathJax_Preview">k_{in}^2 / k_{out}^2</span><script type="math/tex">k_{in}^2 / k_{out}^2</script></span>
where <span><span class="MathJax_Preview">k_{in}</span><script type="math/tex">k_{in}</script></span> is the input kernel size and <span><span class="MathJax_Preview">k_{out}</span><script type="math/tex">k_{out}</script></span> is the targeted kernel size.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Kernels that should be interpolated, of shape (out_channels, in_channels, kernel_size, kernel_size)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Rescaled kernels, of shape (out_channels, in_channels, target_size, target_size)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    &#34;&#34;&#34;
    Interpolates the kernels using the given interpolation method. In addition, it normalizes the rescaled kernel
    such that the outputs remain comparable across scales, i.e. by multiplying with \\(k_{in}^2 / k_{out}^2\\)
    where \\(k_{in}\\) is the input kernel size and \\(k_{out}\\) is the targeted kernel size.

    Parameters
    ----------
    x: torch.Tensor
        Kernels that should be interpolated, of shape (out_channels, in_channels, kernel_size, kernel_size)

    Returns
    -------
    torch.Tensor
        Rescaled kernels, of shape (out_channels, in_channels, target_size, target_size)
    &#34;&#34;&#34;
    if self.mode == &#39;area&#39;:
        out_channels, in_channels, kernel_h, kernel_w = x.shape
        x = x.view(out_channels, in_channels, kernel_h * kernel_w, 1)
        x = torch.matmul(self.interpolation_matrix, x)
        x = x.view(out_channels, in_channels, self.target_size, self.target_size)
    else:
        x = F.interpolate(x, size=(self.target_size, self.target_size), mode=self.mode)
    x *= (self.kernel_size * self.kernel_size) / (self.target_size * self.target_size)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="siconvnet.layers.ScalePool"><code class="flex name class">
<span>class <span class="ident">ScalePool</span></span>
<span>(</span><span>mode:Â strÂ =Â 'pixel')</span>
</code></dt>
<dd>
<div class="desc"><p>Pooling layer to collapse the additional scale dimension.</p>
<p>One of the problems after applying the scale-invariant convolution is the additional dimension representing the
scale. This makes it incompatible with many standard architectures and increases the overall complexity. One way
to handle this is to remove the dimension via pooling. Two pooling strategies are proposed:</p>
<ul>
<li>Pixel-wise Pooling (<code>pixel</code>): This is considered the standard pooling operation, whereby each pixel is pooled
independently of each other over the scale domain. However, this can lead to uncoordinated activations as the
scales for each pixel don't necessarily match or correlate.</li>
<li>Slice-wise Pooling (<code>slice</code>): This is an alternative to the standard pooling operation, where the slice
containing the largest activation is used. Since the scales for each pixel are thus equal, the activations are
better coordinated.</li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mode</code></strong> :&ensp;<code>Literal['pixel', 'slice']</code></dt>
<dd>Method used for pooling. Must be either pixel or slice. For the former, the maximum for each individual pixel
is returned, for the latter the slice with the largest activation is returned.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScalePool(nn.Module):
    &#34;&#34;&#34; Pooling layer to collapse the additional scale dimension.

    One of the problems after applying the scale-invariant convolution is the additional dimension representing the
    scale. This makes it incompatible with many standard architectures and increases the overall complexity. One way
    to handle this is to remove the dimension via pooling. Two pooling strategies are proposed:

    * Pixel-wise Pooling (`pixel`): This is considered the standard pooling operation, whereby each pixel is pooled
    independently of each other over the scale domain. However, this can lead to uncoordinated activations as the
    scales for each pixel don&#39;t necessarily match or correlate.
    * Slice-wise Pooling (`slice`): This is an alternative to the standard pooling operation, where the slice
    containing the largest activation is used. Since the scales for each pixel are thus equal, the activations are
    better coordinated.

    Parameters
    ----------
    mode: Literal[&#39;pixel&#39;, &#39;slice&#39;]
        Method used for pooling. Must be either pixel or slice. For the former, the maximum for each individual pixel
        is returned, for the latter the slice with the largest activation is returned.
    &#34;&#34;&#34;

    def __init__(self, mode: str = &#39;pixel&#39;):
        &#34;&#34;&#34;&#34;&#34;&#34;
        super().__init__()
        if mode not in [&#39;pixel&#39;, &#39;slice&#39;]:
            raise ValueError(&#39;Pooling mode must be either `pixel` or `slice`, got {}&#39;.format(mode))
        self.mode = mode

    def forward(self, x):
        &#34;&#34;&#34; Performs the pooling on the input.

        Parameters
        ----------
        x: torch.Tensor
            Feature maps that should be pooled, of shape (mini_batch, in_channels, num_scales, height, width)

        Returns
        -------
        torch.Tensor
            Pooled feature maps, of shape (mini_batch, in_channels, height, width)
        &#34;&#34;&#34;
        if self.mode == &#39;pixel&#39;:
            num_scales = x.shape[-3]
            return F.max_pool3d(x, kernel_size=(num_scales, 1, 1)).squeeze(dim=-3)
        if self.mode == &#39;slice&#39;:
            num_scales, height, width = x.shape[-3], x.shape[-2], x.shape[-1]
            _, act_index = F.max_pool3d(x, kernel_size=(num_scales, height, width), return_indices=True)
            indices = act_index.repeat(1, 1, 1, height, width) // (height * width)
            return x.gather(dim=-3, index=indices).squeeze(dim=-3)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="siconvnet.layers.ScalePool.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Performs the pooling on the input.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Feature maps that should be pooled, of shape (mini_batch, in_channels, num_scales, height, width)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Pooled feature maps, of shape (mini_batch, in_channels, height, width)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    &#34;&#34;&#34; Performs the pooling on the input.

    Parameters
    ----------
    x: torch.Tensor
        Feature maps that should be pooled, of shape (mini_batch, in_channels, num_scales, height, width)

    Returns
    -------
    torch.Tensor
        Pooled feature maps, of shape (mini_batch, in_channels, height, width)
    &#34;&#34;&#34;
    if self.mode == &#39;pixel&#39;:
        num_scales = x.shape[-3]
        return F.max_pool3d(x, kernel_size=(num_scales, 1, 1)).squeeze(dim=-3)
    if self.mode == &#39;slice&#39;:
        num_scales, height, width = x.shape[-3], x.shape[-2], x.shape[-1]
        _, act_index = F.max_pool3d(x, kernel_size=(num_scales, height, width), return_indices=True)
        indices = act_index.repeat(1, 1, 1, height, width) // (height * width)
        return x.gather(dim=-3, index=indices).squeeze(dim=-3)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="siconvnet.layers.SiConv2d"><code class="flex name class">
<span>class <span class="ident">SiConv2d</span></span>
<span>(</span><span>in_channels:Â int, out_channels:Â int, num_scales:Â int, kernel_size:Â int, stride:Â intÂ =Â 1, padding:Â boolÂ =Â False, interp_mode:Â strÂ =Â 'nearest')</span>
</code></dt>
<dd>
<div class="desc"><p>Scale-invariant convolution layer.</p>
<p>Applies a scale-invariant 2D convolution over an input signal composed of several
input planes. This operation is comparable to a standard convolution, but generates an
additional output dimension representing scale.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>in_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of channels in the input image.</dd>
<dt><strong><code>out_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of channels produced by the convolution.</dd>
<dt><strong><code>num_scales</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of scales the kernels are applied on. Starts with kernel size and then incrementally
increases by 2 on each side. Usually chosen so largest kernel size covers complete input.</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Size of the convolution kernel. Only kernels with the same height and width are supported.</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether zero-padding should be applied to the input. If enabled, significantly more operations
are performed which might degrade performance.</dd>
<dt><strong><code>interp_mode</code></strong> :&ensp;<code>Literal['nearest', 'bilinear', 'bicubic', 'area']</code></dt>
<dd>Method used for kernel interpolation. See <code><a title="siconvnet.layers.Interpolate" href="#siconvnet.layers.Interpolate">Interpolate</a></code> for details.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SiConv2d(nn.Module):
    &#34;&#34;&#34; Scale-invariant convolution layer.

    Applies a scale-invariant 2D convolution over an input signal composed of several
    input planes. This operation is comparable to a standard convolution, but generates an
    additional output dimension representing scale.

    Parameters
    ----------
    in_channels: int
        Number of channels in the input image.
    out_channels: int
        Number of channels produced by the convolution.
    num_scales: int
        Number of scales the kernels are applied on. Starts with kernel size and then incrementally
        increases by 2 on each side. Usually chosen so largest kernel size covers complete input.
    kernel_size: int
        Size of the convolution kernel. Only kernels with the same height and width are supported.
    padding: bool
        Whether zero-padding should be applied to the input. If enabled, significantly more operations
        are performed which might degrade performance.
    interp_mode: Literal[&#39;nearest&#39;, &#39;bilinear&#39;, &#39;bicubic&#39;, &#39;area&#39;]
        Method used for kernel interpolation. See `Interpolate` for details.
    &#34;&#34;&#34;

    def __init__(self, in_channels: int, out_channels: int, num_scales: int, kernel_size: int, stride: int = 1,
                 padding: bool = False, interp_mode: str = &#39;nearest&#39;):
        &#34;&#34;&#34;&#34;&#34;&#34;
        super().__init__()
        # Save all arguments as instance variables
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_scales = num_scales
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.interp_mode = interp_mode
        # Generate weights and bias
        weight = torch.empty((out_channels, in_channels, kernel_size, kernel_size))
        self.weights = nn.Parameter(weight)
        bias = torch.empty((out_channels,))
        self.bias = nn.Parameter(bias)
        # Initialize weights and bias
        stdv = 1.0 / math.sqrt(in_channels * kernel_size * kernel_size)
        self.weights.data.uniform_(-stdv, stdv)
        self.bias.data.uniform_(-stdv, stdv)
        # Initialize interpolation layers
        interpolate = list()
        for i in range(num_scales):
            interpolate.append(Interpolate(kernel_size, kernel_size + 2 * i, mode=interp_mode))
        self.interpolate = nn.ModuleList(interpolate)

    def forward(self, x):
        &#34;&#34;&#34; Performs the scale-invariant convolution on the input.
        &#34;&#34;&#34;
        # Assert shape matches expectations
        _, in_channels, height, width = x.shape
        assert in_channels == self.in_channels, &#39;Expected {} input channels, got {}&#39; \
            .format(self.in_channels, in_channels)
        num_scales = (min(height, width) - self.kernel_size) // 2 + 1
        assert num_scales == self.num_scales, &#39;Expected {} scales, got {}&#39; \
            .format(self.num_scales, num_scales)
        # Apply scale-invariant convolution
        # 1. Generate scaled kernel for each scale
        kernels = list()
        for interpolate in self.interpolate:
            kernels.append(interpolate(self.weights))
        # 2. Compute convolution for each scale
        outputs = list()
        for kernel in kernels:
            if self.padding:
                _padding = kernel.shape[-1] // 2
                output = F.conv2d(x, kernel, self.bias, padding=_padding)
            else:
                output = F.conv2d(x, kernel, self.bias)
                _padding = (kernel.shape[-1] // 2) - (kernels[0].shape[-1] // 2)
                output = F.pad(output, 4 * (_padding,))
            outputs.append(output)
        return torch.stack(outputs, 2)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="siconvnet.layers.SiConv2d.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Performs the scale-invariant convolution on the input.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    &#34;&#34;&#34; Performs the scale-invariant convolution on the input.
    &#34;&#34;&#34;
    # Assert shape matches expectations
    _, in_channels, height, width = x.shape
    assert in_channels == self.in_channels, &#39;Expected {} input channels, got {}&#39; \
        .format(self.in_channels, in_channels)
    num_scales = (min(height, width) - self.kernel_size) // 2 + 1
    assert num_scales == self.num_scales, &#39;Expected {} scales, got {}&#39; \
        .format(self.num_scales, num_scales)
    # Apply scale-invariant convolution
    # 1. Generate scaled kernel for each scale
    kernels = list()
    for interpolate in self.interpolate:
        kernels.append(interpolate(self.weights))
    # 2. Compute convolution for each scale
    outputs = list()
    for kernel in kernels:
        if self.padding:
            _padding = kernel.shape[-1] // 2
            output = F.conv2d(x, kernel, self.bias, padding=_padding)
        else:
            output = F.conv2d(x, kernel, self.bias)
            _padding = (kernel.shape[-1] // 2) - (kernels[0].shape[-1] // 2)
            output = F.pad(output, 4 * (_padding,))
        outputs.append(output)
    return torch.stack(outputs, 2)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="siconvnet" href="index.html">siconvnet</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="siconvnet.layers.Interpolate" href="#siconvnet.layers.Interpolate">Interpolate</a></code></h4>
<ul class="">
<li><code><a title="siconvnet.layers.Interpolate.forward" href="#siconvnet.layers.Interpolate.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="siconvnet.layers.ScalePool" href="#siconvnet.layers.ScalePool">ScalePool</a></code></h4>
<ul class="">
<li><code><a title="siconvnet.layers.ScalePool.forward" href="#siconvnet.layers.ScalePool.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="siconvnet.layers.SiConv2d" href="#siconvnet.layers.SiConv2d">SiConv2d</a></code></h4>
<ul class="">
<li><code><a title="siconvnet.layers.SiConv2d.forward" href="#siconvnet.layers.SiConv2d.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>